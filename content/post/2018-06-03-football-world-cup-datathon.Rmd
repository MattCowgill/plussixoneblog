---
title: Football World Cup Datathon
author: James Day
date: '2018-06-03'
slug: football-world-cup-datathon
categories:
  - Football
tags: []
draft: true
---

# Introduction
some text


# Setup
First lets load some packages and get our data

```{r setup}
library(pacman)
p_load(tidyverse, here, caret, MLmetrics)

```

# Data exploration
```{r load-data}
dat <- read_csv(here::here("data", "world-cup", "wc_datathon_dataset.csv"))
head(dat)
tail(dat)
glimpse(dat)
```

So it looks like Betafair has given us the match results, home team, the tournament and the pre-match betfair odds for all games dating back to 2000. That's a decent start! As a minimum, I should be able to create a simple ELO model from that. In subsequent posts, I am going to explore combining that with the odds and some other data as inputs into some machine learning models to see how that looks.

Before I start, I just want to see if there are any NA's as this can cause problems with lots of models.

One thing I did notice is that

```{r}
dat %>% 
  summarise_each(funs(sum(is.na(.))))
```
So it looks like we are missing around 5700 matches betting odds from a total of 14000. That might be an issue for a model that wants to use betting odds...

For now, let's filter them out and I'll see if we can either retreive them or decide to not use that as an input.

```{r clean}
dat <- dat %>% 
  na.omit
```

# Baseline Model
Before starting - I think I'm going to take the implied probability from betfair as the baseline model. Any model I implement needs to be an improvement on that! 

## Data splitting
In order to test any models we use, I'm going to split our data into training and testing data. This is important in that it gives us a way to test our models without including that testing data in the information we give our models - in essence, we can assess the models performance on "unseen" data. 

Typically, this is done by taking a random sample of 20% of the data as the test data, and only training your model on the remaining 80%. I'm not sure if this is a good approach for time based models but I'm going to do it for now. 

```{r data split}
set.seed(42)
dat <- dat %>%
  mutate(id = row_number())

train_index <- createDataPartition(dat$id, p = 0.8, list = F)

dat_train <- dat[train_index, ]
dat_test <- dat[-train_index, ]
```

Now to take the implied odds as my simple model - this is done by taking the 1/odds for each of the main outcomes - win, draw and loss - for team 1.

```{r model}
dat_test <- dat_test %>%
  mutate(win = 1/team_1_betfair_odds,
         draw = 1/draw_betfair_odds,
         lose = 1/team_2_betfair_odds)

# They each need to add to 1
dat_test <- dat_test %>%
  mutate(sum = win + draw + lose) %>%
  mutate_at(c("win", "draw", "lose"), funs(./sum))
```

Now to calculate the `log_loss` value for our model. This is how the winner of the competition will be assessed and so it's what I'll base all of my models training on. In order to do this, I need the actual outcome as a column as well. 

```{r logloss}
dat_test <- dat_test %>%
  mutate(outcome = factor(case_when(
    team_1_goals > team_2_goals ~ "win",
    team_1_goals < team_2_goals ~ "lose",
    TRUE ~ "draw"
  )))

pred <- dat_test %>%
  select(win:lose) %>%
  as.matrix()

actual <- dat_test$outcome

MultiLogLoss(y_pred = pred, y_true = dat_test$outcome)
```
